{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    \n",
    "    def __init__(self,in_shape,unit,out_shape):\n",
    "        '''\n",
    "        in_shape:输入向量的大小\n",
    "        unit:隐层大小\n",
    "        out_shape:输出向量的大小\n",
    "        '''\n",
    "        self.U = np.random.random(size=(in_shape,unit)) #输入层到隐层的权重\n",
    "        self.W = np.random.random(size=(unit,unit))\n",
    "        self.V = np.random.random(size=(unit,out_shape))\n",
    "        \n",
    "        self.in_shape = in_shape\n",
    "        self.unit = unit\n",
    "        self.out_shape = out_shape\n",
    "        \n",
    "        self.start_h = np.random.random(size=(self.unit,)) #初试隐层的状态\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh(x):\n",
    "        return (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\n",
    "    \n",
    "    @staticmethod\n",
    "    def tanh_der(y):\n",
    "        return 1 - y*y\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(x):\n",
    "        tmp = np.exp(x)\n",
    "        return tmp/sum(tmp)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax_der(y,y_):\n",
    "        j = np.argmax(y_)\n",
    "        tmp = y[j]\n",
    "        y = -y[j]*y\n",
    "        y[j] = tmp*(1-tmp)\n",
    "        return y\n",
    "    \n",
    "    @staticmethod\n",
    "    def cross_entropy(y, y_):\n",
    "        '''\n",
    "        交叉熵\n",
    "        y:预测值\n",
    "        y_: 真值\n",
    "        '''\n",
    "        return sum(-np.log(y)*y_)\n",
    "    \n",
    "    @staticmethod\n",
    "    def cross_entropy_der(y, y_):\n",
    "        j = np.argmax(y_)\n",
    "        return -1/y[j]\n",
    "    \n",
    "    def inference(self, x, h_1):\n",
    "        '''\n",
    "        前向传播\n",
    "        x: 输入向量\n",
    "        h_1: 上一隐层\n",
    "\n",
    "        '''\n",
    "        h = self.tanh(np.dot(x, self.U) + np.dot(h_1, self.W))\n",
    "        y = self.softmax(np.dot(h, self.V))\n",
    "        return h, y\n",
    "    \n",
    "    def train(self, x_data, y_data, alpha=0.1, steps=100):\n",
    "        '''\n",
    "        训练模型\n",
    "        x_data: 输入样本\n",
    "        y_data: 标签\n",
    "        alpha: 学习率\n",
    "        steps: 迭代伦次\n",
    "        '''\n",
    "        for step in range(steps):  # 迭代伦次\n",
    "            print(\"step:\", step+1)\n",
    "            for xs, ys in zip(x_data,y_data):  # 每个样本\n",
    "                h_list = []\n",
    "                h = self.start_h  # 初始化初始隐层状态\n",
    "                h_list.append(h)\n",
    "                y_list = []\n",
    "                losses = []\n",
    "                for x, y_ in zip(xs, ys):  # 前向传播\n",
    "                    h, y = self.inference(x, h)\n",
    "                    loss = self.cross_entropy(y=y, y_=y_)\n",
    "                    h_list.append(h)\n",
    "                    y_list.append(y)\n",
    "                    losses.append(loss)\n",
    "                print(\"loss:\", np.mean(losses))\n",
    "                V_update = np.zeros(shape=self.V.shape)\n",
    "                U_update = np.zeros(shape=self.U.shape)\n",
    "                W_update = np.zeros(shape=self.W.shape)\n",
    "                next_layer1_delta = np.zeros(shape=(self.unit,))\n",
    "\n",
    "                for i in range(len(xs))[::-1]:  # 反向传播\n",
    "                    layer2_delta = -self.cross_entropy_der(y_list[i], ys[i])*self.softmax_der(y_list[i], ys[i])  # 输出层误差\n",
    "                    # 当前隐层梯度 = 下一隐层梯度 * 下一隐层权重 + 输出层梯度 * 输出层权重\n",
    "                    layer1_delta = self.tanh_der(h_list[i+1])*(np.dot(layer2_delta, self.V.T) + np.dot(next_layer1_delta, self.W.T))\n",
    "\n",
    "                    V_update += np.dot(np.atleast_2d(h_list[i+1]).T, np.atleast_2d(layer2_delta))  # V增量\n",
    "                    W_update += np.dot(np.atleast_2d(h_list[i]).T,  np.atleast_2d(layer1_delta))  # W增量\n",
    "                    U_update += np.dot(np.atleast_2d(xs[i]).T,  np.atleast_2d(layer1_delta))  # U增量\n",
    "\n",
    "                    next_layer1_delta = layer1_delta  # 更新下一隐层的梯度等于当前隐层的梯度\n",
    "                self.W += W_update * alpha\n",
    "                self.V += V_update * alpha\n",
    "                self.U += U_update * alpha\n",
    "    def predict(self, xs, return_sequence=False):\n",
    "        '''\n",
    "        RNN预测\n",
    "        xs: 单个样本\n",
    "        :param return_sequence: 是否返回整个输出序列\n",
    "        '''\n",
    "\n",
    "        y_list = []\n",
    "        h_list = []\n",
    "        h = self.start_h\n",
    "        for x in xs:\n",
    "            h, y = self.inference(x,h)\n",
    "            y_list.append(y)\n",
    "            h_list.append(h)\n",
    "        if return_sequence:\n",
    "            return h_list, y_list\n",
    "        else:\n",
    "            return h_list[-1], y_list[-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 1\n",
      "loss: 2.077201875224448\n",
      "loss: 1.8894262844484704\n",
      "loss: 2.417838391008928\n",
      "loss: 3.331204131167494\n",
      "loss: 2.531513100410853\n",
      "loss: 2.2405261910608543\n",
      "loss: 2.0387999920226787\n",
      "step: 2\n",
      "loss: 1.8554869779839755\n",
      "loss: 1.821915883080802\n",
      "loss: 2.2371012613526706\n",
      "loss: 3.178030395799348\n",
      "loss: 2.617270814803658\n",
      "loss: 2.2487265809471104\n",
      "loss: 2.0154752203625677\n",
      "step: 3\n",
      "loss: 1.8534320824298367\n",
      "loss: 1.8157543637997067\n",
      "loss: 2.2239464827825373\n",
      "loss: 3.146247349059942\n",
      "loss: 2.612266936597083\n",
      "loss: 2.2467253986396867\n",
      "loss: 2.010165913591942\n",
      "step: 4\n",
      "loss: 1.8499162094825221\n",
      "loss: 1.8119167415329376\n",
      "loss: 2.220083818214011\n",
      "loss: 3.130581571049366\n",
      "loss: 2.605796312706854\n",
      "loss: 2.244405136816895\n",
      "loss: 2.007365557294827\n",
      "step: 5\n",
      "loss: 1.847032556935157\n",
      "loss: 1.809008929396873\n",
      "loss: 2.2181149113262455\n",
      "loss: 3.1220121859838286\n",
      "loss: 2.5984399085399494\n",
      "loss: 2.241832076015241\n",
      "loss: 2.005078214374914\n",
      "step: 6\n",
      "loss: 1.8443001339557046\n",
      "loss: 1.8063983358956515\n",
      "loss: 2.216940988311771\n",
      "loss: 3.116723211240797\n",
      "loss: 2.5894794355972253\n",
      "loss: 2.238946126481293\n",
      "loss: 2.002839854787246\n",
      "step: 7\n",
      "loss: 1.8412810218641882\n",
      "loss: 1.8037590894090876\n",
      "loss: 2.2162710563901933\n",
      "loss: 3.112966475542315\n",
      "loss: 2.5778247710278883\n",
      "loss: 2.235622961097924\n",
      "loss: 2.0003344523210296\n",
      "step: 8\n",
      "loss: 1.8374499756906166\n",
      "loss: 1.8008164430018716\n",
      "loss: 2.2160165248120727\n",
      "loss: 3.109883574562638\n",
      "loss: 2.5615079525843316\n",
      "loss: 2.2316322097231973\n",
      "loss: 1.9971140948071229\n",
      "step: 9\n",
      "loss: 1.8318954156408145\n",
      "loss: 1.797216119248006\n",
      "loss: 2.2162069140941574\n",
      "loss: 3.106928167871928\n",
      "loss: 2.536492751629556\n",
      "loss: 2.226564510370461\n",
      "loss: 1.9922882346267101\n",
      "step: 10\n",
      "loss: 1.8225575791218\n",
      "loss: 1.7923303092978957\n",
      "loss: 2.21703131773011\n",
      "loss: 3.1035258614886296\n",
      "loss: 2.493137917534614\n",
      "loss: 2.2196023651477867\n",
      "loss: 1.9835918704830238\n",
      "step: 11\n",
      "loss: 1.8035934690410793\n",
      "loss: 1.784768042787863\n",
      "loss: 2.2189956068899974\n",
      "loss: 3.0986233173731184\n",
      "loss: 2.4039296852466734\n",
      "loss: 2.208836140526123\n",
      "loss: 1.9638374186713277\n",
      "step: 12\n",
      "loss: 1.7547756313049634\n",
      "loss: 1.7715705012009024\n",
      "loss: 2.222147088429738\n",
      "loss: 3.08937134053673\n",
      "loss: 2.184060127280315\n",
      "loss: 2.1903819612645132\n",
      "loss: 1.9103810349463426\n",
      "step: 13\n",
      "loss: 1.624178558332093\n",
      "loss: 1.7551230665148019\n",
      "loss: 2.2006371708122865\n",
      "loss: 3.0618973448514786\n",
      "loss: 1.7666429965336958\n",
      "loss: 2.165982795560925\n",
      "loss: 1.8192261307414783\n",
      "step: 14\n",
      "loss: 1.5322337488196283\n",
      "loss: 1.6974914601924302\n",
      "loss: 2.1284495988116703\n",
      "loss: 2.9923115004699166\n",
      "loss: 1.489154958037933\n",
      "loss: 2.144611337664865\n",
      "loss: 1.753245703638424\n",
      "step: 15\n",
      "loss: 1.4202133034787339\n",
      "loss: 1.6516950404536022\n",
      "loss: 2.0987122426862994\n",
      "loss: 2.976953396706866\n",
      "loss: 1.1171433138510445\n",
      "loss: 2.1029780732220527\n",
      "loss: 1.6941973309454295\n",
      "step: 16\n",
      "loss: 1.337497094547188\n",
      "loss: 1.6045339823185252\n",
      "loss: 2.0649182419859082\n",
      "loss: 2.9606225121276757\n",
      "loss: 0.8665328049598907\n",
      "loss: 2.054740480709809\n",
      "loss: 1.647472838975209\n",
      "step: 17\n",
      "loss: 1.2734110109264385\n",
      "loss: 1.5551700210271442\n",
      "loss: 2.029985273612813\n",
      "loss: 2.9439065330360648\n",
      "loss: 0.7043562808247195\n",
      "loss: 1.9924310863213623\n",
      "loss: 1.6070382726752153\n",
      "step: 18\n",
      "loss: 1.2201672999422828\n",
      "loss: 1.5011680488979016\n",
      "loss: 1.9914916222965044\n",
      "loss: 2.925433204699454\n",
      "loss: 0.6022072121230928\n",
      "loss: 1.9050526923986904\n",
      "loss: 1.5688309134896292\n",
      "step: 19\n",
      "loss: 1.1708769734264617\n",
      "loss: 1.4378060653652767\n",
      "loss: 1.9466377307297902\n",
      "loss: 2.9032563340817554\n",
      "loss: 0.5381739426003419\n",
      "loss: 1.7788200689887281\n",
      "loss: 1.5275589253733919\n",
      "step: 20\n",
      "loss: 1.1158089177997494\n",
      "loss: 1.3654036129929406\n",
      "loss: 1.8900657947293316\n",
      "loss: 2.8707084569266637\n",
      "loss: 0.4860357589152732\n",
      "loss: 1.5995896724885965\n",
      "loss: 1.4712306950123935\n",
      "step: 21\n",
      "loss: 1.0841990418111833\n",
      "loss: 1.360571211164211\n",
      "loss: 1.8217102423390257\n",
      "loss: 2.819832735825891\n",
      "loss: 0.3782359809314757\n",
      "loss: 1.456449427432577\n",
      "loss: 1.4703273934990853\n",
      "step: 22\n",
      "loss: 0.8698275084423832\n",
      "loss: 1.1884453579167313\n",
      "loss: 1.8069421342092244\n",
      "loss: 2.801093996406164\n",
      "loss: 0.27292258666200037\n",
      "loss: 1.262739626235041\n",
      "loss: 1.3593163983409677\n",
      "step: 23\n",
      "loss: 0.7691704262797364\n",
      "loss: 1.1196185834982963\n",
      "loss: 1.7679539306188876\n",
      "loss: 2.735019812032269\n",
      "loss: 0.2096183539813508\n",
      "loss: 1.1532986187272318\n",
      "loss: 1.2866317952348945\n",
      "step: 24\n",
      "loss: 0.6587442277395965\n",
      "loss: 1.0495534357840437\n",
      "loss: 1.7361542786862865\n",
      "loss: 2.613896097029808\n",
      "loss: 0.171007302278688\n",
      "loss: 1.032178639003432\n",
      "loss: 1.1951057612565203\n",
      "step: 25\n",
      "loss: 0.5492105087631046\n",
      "loss: 0.9378986954429432\n",
      "loss: 1.656541333618083\n",
      "loss: 2.5103118874606243\n",
      "loss: 0.16391298601479207\n",
      "loss: 0.8407375165987622\n",
      "loss: 1.0909293030280445\n",
      "step: 26\n",
      "loss: 0.45235718116334045\n",
      "loss: 0.8148252396214238\n",
      "loss: 1.5508687302973156\n",
      "loss: 2.4780345454273203\n",
      "loss: 0.16058221684391297\n",
      "loss: 0.6687107770299469\n",
      "loss: 1.0019673560711233\n",
      "step: 27\n",
      "loss: 0.37612983315405923\n",
      "loss: 0.7228185919286298\n",
      "loss: 1.4511615354794138\n",
      "loss: 2.445506031615072\n",
      "loss: 0.1543965923306235\n",
      "loss: 0.5474941259804049\n",
      "loss: 0.935116582447706\n",
      "step: 28\n",
      "loss: 0.32046770873396224\n",
      "loss: 0.6616089607097102\n",
      "loss: 1.3577964937359446\n",
      "loss: 2.405103988667358\n",
      "loss: 0.14778795163748149\n",
      "loss: 0.4628672693531672\n",
      "loss: 0.8841485877295691\n",
      "step: 29\n",
      "loss: 0.28212793939357883\n",
      "loss: 0.6203621579025055\n",
      "loss: 1.2702859043441557\n",
      "loss: 2.3581248621571294\n",
      "loss: 0.1423204764534987\n",
      "loss: 0.39865268159687123\n",
      "loss: 0.8446995746357864\n",
      "step: 30\n",
      "loss: 0.27120533383005957\n",
      "loss: 0.593534560049768\n",
      "loss: 1.1987998122798946\n",
      "loss: 2.3127876267212817\n",
      "loss: 0.13697170031127417\n",
      "loss: 0.34470153321194774\n",
      "loss: 0.815318413078555\n",
      "step: 31\n",
      "loss: 0.300369761210404\n",
      "loss: 0.5805344033091772\n",
      "loss: 1.1488480807224177\n",
      "loss: 2.274700377895847\n",
      "loss: 0.13287295103495814\n",
      "loss: 0.3033944824515246\n",
      "loss: 0.7824803027449899\n",
      "step: 32\n",
      "loss: 0.2143429840682326\n",
      "loss: 0.538769636036511\n",
      "loss: 1.0791833640591988\n",
      "loss: 2.214693026395239\n",
      "loss: 0.1297143914203078\n",
      "loss: 0.2644776271809121\n",
      "loss: 0.7882493849739524\n",
      "step: 33\n",
      "loss: 0.7269271812579587\n",
      "loss: 0.5753349667206025\n",
      "loss: 1.108787936550879\n",
      "loss: 2.18686743110568\n",
      "loss: 0.16074946249242045\n",
      "loss: 0.2792473039726523\n",
      "loss: 0.8126556269886351\n",
      "step: 34\n",
      "loss: 0.19794284131820664\n",
      "loss: 0.4799933562379935\n",
      "loss: 1.063448862223611\n",
      "loss: 2.125639253931961\n",
      "loss: 0.11069610291107528\n",
      "loss: 0.2357519350647943\n",
      "loss: 0.762807256689842\n",
      "step: 35\n",
      "loss: 0.149854711964811\n",
      "loss: 0.4666741052229946\n",
      "loss: 1.0270649303258765\n",
      "loss: 2.0870836096482037\n",
      "loss: 0.09230524872660767\n",
      "loss: 0.2173871297548216\n",
      "loss: 0.7286151573531655\n",
      "step: 36\n",
      "loss: 0.13049843903680838\n",
      "loss: 0.44866308623133816\n",
      "loss: 0.9767717903267714\n",
      "loss: 2.035645935756335\n",
      "loss: 0.08594315593432901\n",
      "loss: 0.201944931586824\n",
      "loss: 0.6953165931129962\n",
      "step: 37\n",
      "loss: 0.12684780347578037\n",
      "loss: 0.43747340829654496\n",
      "loss: 0.919212512243083\n",
      "loss: 2.0110846678811387\n",
      "loss: 0.08973242925497656\n",
      "loss: 0.19275863510564695\n",
      "loss: 0.6701148960641711\n",
      "step: 38\n",
      "loss: 0.19557177087481178\n",
      "loss: 0.4864859352012223\n",
      "loss: 0.9399194438270577\n",
      "loss: 2.002547523530114\n",
      "loss: 0.09255754121223889\n",
      "loss: 0.1826983778484397\n",
      "loss: 0.6512614276149961\n",
      "step: 39\n",
      "loss: 0.1504033201753366\n",
      "loss: 0.4273102151125927\n",
      "loss: 0.8689103408868054\n",
      "loss: 1.9080026031513615\n",
      "loss: 0.10981394202674076\n",
      "loss: 0.2020828987852508\n",
      "loss: 0.6419419781121886\n",
      "step: 40\n",
      "loss: 0.10126515404197933\n",
      "loss: 0.40252592311429\n",
      "loss: 0.8299694633617647\n",
      "loss: 1.8222755339799992\n",
      "loss: 0.14433463814774833\n",
      "loss: 0.17616390347274302\n",
      "loss: 0.6589448008277966\n",
      "step: 41\n",
      "loss: 0.15420013917177716\n",
      "loss: 0.36775011933398316\n",
      "loss: 0.807911660891349\n",
      "loss: 1.7823866423486603\n",
      "loss: 0.12485119933093002\n",
      "loss: 0.21426120929700818\n",
      "loss: 0.5815245676660223\n",
      "step: 42\n",
      "loss: 0.10895393030999008\n",
      "loss: 0.3414098622001549\n",
      "loss: 0.7444446484206958\n",
      "loss: 1.7060725144793736\n",
      "loss: 0.11365895319110966\n",
      "loss: 0.1154481449164872\n",
      "loss: 0.8075948949709578\n",
      "step: 43\n",
      "loss: 2.356230219704574\n",
      "loss: 0.5136708043816972\n",
      "loss: 0.8294774939089679\n",
      "loss: 1.7448582172719636\n",
      "loss: 0.07809524347293693\n",
      "loss: 0.24285840442804224\n",
      "loss: 0.7264331931511988\n",
      "step: 44\n",
      "loss: 0.290668861479258\n",
      "loss: 0.28964298134900734\n",
      "loss: 0.8018371420415438\n",
      "loss: 1.6755931488604248\n",
      "loss: 0.06254954434093077\n",
      "loss: 0.1503135477080772\n",
      "loss: 0.6438939058411092\n",
      "step: 45\n",
      "loss: 0.25851767713522505\n",
      "loss: 0.27695535474648114\n",
      "loss: 0.7372888133146148\n",
      "loss: 1.6342957791576904\n",
      "loss: 0.05620509226031278\n",
      "loss: 0.15387237048134855\n",
      "loss: 0.661761545866955\n",
      "step: 46\n",
      "loss: 0.24596338704020645\n",
      "loss: 0.2577541931971627\n",
      "loss: 0.9025849567674128\n",
      "loss: 1.6595509341115449\n",
      "loss: 0.052730925640502\n",
      "loss: 0.16116577621067643\n",
      "loss: 0.6205211772453452\n",
      "step: 47\n",
      "loss: 0.2614086676694804\n",
      "loss: 0.25192738081050525\n",
      "loss: 0.7679357910605824\n",
      "loss: 1.548268506083865\n",
      "loss: 0.05103188234820386\n",
      "loss: 0.1648870104934579\n",
      "loss: 0.7130356545484355\n",
      "step: 48\n",
      "loss: 0.2537968187539205\n",
      "loss: 0.24445001114354983\n",
      "loss: 0.7606694463495683\n",
      "loss: 1.511793084564838\n",
      "loss: 0.047913438406238476\n",
      "loss: 0.15364156289928882\n",
      "loss: 0.6887312736650647\n",
      "step: 49\n",
      "loss: 0.2478021874827809\n",
      "loss: 0.24303582027769757\n",
      "loss: 0.7188014490045647\n",
      "loss: 1.4701815181747608\n",
      "loss: 0.0454367683228529\n",
      "loss: 0.14237379304206932\n",
      "loss: 0.6663139184478293\n",
      "step: 50\n",
      "loss: 0.24075289211391693\n",
      "loss: 0.23769433474788207\n",
      "loss: 0.690871949013335\n",
      "loss: 1.413174899092086\n",
      "loss: 0.04359385061677413\n",
      "loss: 0.12330496189831841\n",
      "loss: 0.6280296429145134\n",
      "step: 51\n",
      "loss: 0.23294641446676276\n",
      "loss: 0.2337794074982363\n",
      "loss: 0.6640865398597944\n",
      "loss: 1.3637747564158156\n",
      "loss: 0.04265079890899797\n",
      "loss: 0.11004426811847723\n",
      "loss: 0.5865381454524902\n",
      "step: 52\n",
      "loss: 0.1984765355279804\n",
      "loss: 0.2292430252830532\n",
      "loss: 0.6346033090416706\n",
      "loss: 1.3025582626699677\n",
      "loss: 0.04277145050156583\n",
      "loss: 0.10346836656801056\n",
      "loss: 0.493393608500067\n",
      "step: 53\n",
      "loss: 0.09218339670864135\n",
      "loss: 0.23186102268795464\n",
      "loss: 0.5772539175758407\n",
      "loss: 1.2023084083405629\n",
      "loss: 0.04566277198622653\n",
      "loss: 0.10718174097447608\n",
      "loss: 0.35306397612400725\n",
      "step: 54\n",
      "loss: 0.0760394507222295\n",
      "loss: 0.22174826022135818\n",
      "loss: 0.4588633012707527\n",
      "loss: 1.1130726103323425\n",
      "loss: 0.050068366597682934\n",
      "loss: 0.10703910556251746\n",
      "loss: 0.3153530135882297\n",
      "step: 55\n",
      "loss: 0.07059921156676364\n",
      "loss: 0.20594592261337283\n",
      "loss: 0.4058815263535472\n",
      "loss: 1.050660410474389\n",
      "loss: 0.05570226195783188\n",
      "loss: 0.10034458772339055\n",
      "loss: 0.28614622815935814\n",
      "step: 56\n",
      "loss: 0.06729939585163804\n",
      "loss: 0.1907330857905519\n",
      "loss: 0.37392121542257795\n",
      "loss: 0.9963941361478169\n",
      "loss: 0.061275816523879256\n",
      "loss: 0.09349844431076156\n",
      "loss: 0.2640942601228709\n",
      "step: 57\n",
      "loss: 0.06461395162900063\n",
      "loss: 0.17689974849734977\n",
      "loss: 0.351457347912758\n",
      "loss: 0.9472299611044835\n",
      "loss: 0.06613517160953128\n",
      "loss: 0.0874224299562629\n",
      "loss: 0.24508929191970977\n",
      "step: 58\n",
      "loss: 0.06263956166748112\n",
      "loss: 0.16538457667485515\n",
      "loss: 0.32852585178918625\n",
      "loss: 0.9016065958041365\n",
      "loss: 0.07014241330380602\n",
      "loss: 0.08100943614331602\n",
      "loss: 0.23022955901694928\n",
      "step: 59\n",
      "loss: 0.06046389783713583\n",
      "loss: 0.15639197741055597\n",
      "loss: 0.3089227002543629\n",
      "loss: 0.8591162054279927\n",
      "loss: 0.07357970082871687\n",
      "loss: 0.07472513855908103\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.2178461988474669\n",
      "step: 60\n",
      "loss: 0.0578560222196527\n",
      "loss: 0.14835902454310002\n",
      "loss: 0.29282695118580576\n",
      "loss: 0.8182862688845204\n",
      "loss: 0.07606127715595178\n",
      "loss: 0.06917712320884407\n",
      "loss: 0.20608633472786075\n",
      "step: 61\n",
      "loss: 0.055357449727782164\n",
      "loss: 0.1412990868265204\n",
      "loss: 0.2772068756344768\n",
      "loss: 0.7760221178921439\n",
      "loss: 0.07762756681159715\n",
      "loss: 0.0640818629047245\n",
      "loss: 0.1948359568295969\n",
      "step: 62\n",
      "loss: 0.05284859767477713\n",
      "loss: 0.13478508970176759\n",
      "loss: 0.2624623150223789\n",
      "loss: 0.7264200007892092\n",
      "loss: 0.07859723231375731\n",
      "loss: 0.059622146595624814\n",
      "loss: 0.1835809352552673\n",
      "step: 63\n",
      "loss: 0.050470396633115165\n",
      "loss: 0.1283616036701499\n",
      "loss: 0.24834143407172388\n",
      "loss: 0.6579353249036873\n",
      "loss: 0.0792252212695042\n",
      "loss: 0.056052777793461836\n",
      "loss: 0.1709012610149636\n",
      "step: 64\n",
      "loss: 0.04832048693231625\n",
      "loss: 0.1213215740959308\n",
      "loss: 0.23400757772896438\n",
      "loss: 0.559904940367198\n",
      "loss: 0.08050421412625108\n",
      "loss: 0.05379800705320302\n",
      "loss: 0.15448362881782354\n",
      "step: 65\n",
      "loss: 0.04638482978338967\n",
      "loss: 0.1125649572298945\n",
      "loss: 0.2186405316370496\n",
      "loss: 0.4652199173113189\n",
      "loss: 0.0808575470471069\n",
      "loss: 0.051622272072710695\n",
      "loss: 0.13674016326442126\n",
      "step: 66\n",
      "loss: 0.044941220642495644\n",
      "loss: 0.10385903943053187\n",
      "loss: 0.202456657880511\n",
      "loss: 0.40186284232249697\n",
      "loss: 0.07853356723262211\n",
      "loss: 0.04840715648414679\n",
      "loss: 0.12333219431208127\n",
      "step: 67\n",
      "loss: 0.04357199372949485\n",
      "loss: 0.09696491431578953\n",
      "loss: 0.18733386540170838\n",
      "loss: 0.35399319449354866\n",
      "loss: 0.07669161770954266\n",
      "loss: 0.045904210261602796\n",
      "loss: 0.11320355035105494\n",
      "step: 68\n",
      "loss: 0.04205613503174043\n",
      "loss: 0.09046700984007953\n",
      "loss: 0.17366660761405125\n",
      "loss: 0.31564535328067106\n",
      "loss: 0.07319613521990376\n",
      "loss: 0.043133766058614915\n",
      "loss: 0.1051208889511776\n",
      "step: 69\n",
      "loss: 0.04051019969025608\n",
      "loss: 0.08525945444441885\n",
      "loss: 0.16156924865010241\n",
      "loss: 0.28456421815724126\n",
      "loss: 0.07128428571207626\n",
      "loss: 0.04145137380441809\n",
      "loss: 0.09776949100617645\n",
      "step: 70\n",
      "loss: 0.038823904812086064\n",
      "loss: 0.07959103149904727\n",
      "loss: 0.15046298555952195\n",
      "loss: 0.2589754201164887\n",
      "loss: 0.06779938300913603\n",
      "loss: 0.03922182457450822\n",
      "loss: 0.09157231578694981\n",
      "step: 71\n",
      "loss: 0.03722071016811839\n",
      "loss: 0.07516511304862547\n",
      "loss: 0.1409435782298687\n",
      "loss: 0.23775354214946\n",
      "loss: 0.06575557128253291\n",
      "loss: 0.037745379539399215\n",
      "loss: 0.08587975658159858\n",
      "step: 72\n",
      "loss: 0.03577379700384369\n",
      "loss: 0.07057256221750624\n",
      "loss: 0.13217584631927218\n",
      "loss: 0.21987122981277119\n",
      "loss: 0.06272855752368758\n",
      "loss: 0.035874742017296665\n",
      "loss: 0.08100613771558991\n",
      "step: 73\n",
      "loss: 0.03440962340178196\n",
      "loss: 0.06684988282544464\n",
      "loss: 0.12451185402894314\n",
      "loss: 0.20462467606592927\n",
      "loss: 0.06048039896287331\n",
      "loss: 0.034424505031410604\n",
      "loss: 0.0765603107234421\n",
      "step: 74\n",
      "loss: 0.033196414348948655\n",
      "loss: 0.06320854388754749\n",
      "loss: 0.1174184755097234\n",
      "loss: 0.19146026169412098\n",
      "loss: 0.05794571737881474\n",
      "loss: 0.03289867879075193\n",
      "loss: 0.07261821169920905\n",
      "step: 75\n",
      "loss: 0.03204035608689571\n",
      "loss: 0.060027351485665494\n",
      "loss: 0.11105317020104484\n",
      "loss: 0.17996048926795402\n",
      "loss: 0.055734235256029586\n",
      "loss: 0.031572772536082025\n",
      "loss: 0.06901576861569862\n",
      "step: 76\n",
      "loss: 0.030986779711970603\n",
      "loss: 0.057050451824544914\n",
      "loss: 0.10518944103082586\n",
      "loss: 0.1698339581945649\n",
      "loss: 0.05356616660099003\n",
      "loss: 0.030304087654245856\n",
      "loss: 0.06574864350250141\n",
      "step: 77\n",
      "loss: 0.030000292323664506\n",
      "loss: 0.05435178381819803\n",
      "loss: 0.09983151029304502\n",
      "loss: 0.16083356468031904\n",
      "loss: 0.05156338642615748\n",
      "loss: 0.029143771567347904\n",
      "loss: 0.06275439496901489\n",
      "step: 78\n",
      "loss: 0.029087824320039937\n",
      "loss: 0.05186538947299948\n",
      "loss: 0.0948918520952304\n",
      "loss: 0.1527806404421446\n",
      "loss: 0.049682241896000144\n",
      "loss: 0.028064107237475242\n",
      "loss: 0.0600069533726689\n",
      "step: 79\n",
      "loss: 0.02823798993836524\n",
      "loss: 0.04958302428370423\n",
      "loss: 0.0903343168593222\n",
      "loss: 0.14552539010780094\n",
      "loss: 0.047933408910453466\n",
      "loss: 0.027065922552882843\n",
      "loss: 0.05747516500852143\n",
      "step: 80\n",
      "loss: 0.027446792251962726\n",
      "loss: 0.04748058283838064\n",
      "loss: 0.08611562309376913\n",
      "loss: 0.13895094935356792\n",
      "loss: 0.04630869548628535\n",
      "loss: 0.026141360360181144\n",
      "loss: 0.055135894776294304\n",
      "step: 81\n",
      "loss: 0.026708529044324632\n",
      "loss: 0.04554157376344384\n",
      "loss: 0.08220316146696467\n",
      "loss: 0.13296124040631055\n",
      "loss: 0.04480288161574465\n",
      "loss: 0.025285133488945007\n",
      "loss: 0.052968581226998435\n",
      "step: 82\n",
      "loss: 0.02601847450162097\n",
      "loss: 0.04375009562969967\n",
      "loss: 0.07856811534775461\n",
      "loss: 0.12747792932532323\n",
      "loss: 0.04340874375716551\n",
      "loss: 0.024491654994448765\n",
      "loss: 0.05095574371132755\n",
      "step: 83\n",
      "loss: 0.025372120323097797\n",
      "loss: 0.042092186857969\n",
      "loss: 0.07518587300730253\n",
      "loss: 0.12243621459931024\n",
      "loss: 0.042118481691428115\n",
      "loss: 0.023755706065540894\n",
      "loss: 0.049082159391386715\n",
      "step: 84\n",
      "loss: 0.024765282139196165\n",
      "loss: 0.040555280027259916\n",
      "loss: 0.07203493786035856\n",
      "loss: 0.1177821657213115\n",
      "loss: 0.040923801579266\n",
      "loss: 0.02307232964780661\n",
      "loss: 0.04733450866199896\n",
      "step: 85\n",
      "loss: 0.024194051392378634\n",
      "loss: 0.03912808066063598\n",
      "loss: 0.06909635112842834\n",
      "loss: 0.1134705565342828\n",
      "loss: 0.03981620711608254\n",
      "loss: 0.022436859367379658\n",
      "loss: 0.04570105197789979\n",
      "step: 86\n",
      "loss: 0.02365479360521967\n",
      "loss: 0.0378004122727019\n",
      "loss: 0.06635319576905122\n",
      "loss: 0.10946320390471054\n",
      "loss: 0.0387872397956967\n",
      "loss: 0.02184492484564035\n",
      "loss: 0.04417138803343773\n",
      "step: 87\n",
      "loss: 0.023144153891542424\n",
      "loss: 0.036563094352597254\n",
      "loss: 0.06379022300605816\n",
      "loss: 0.10572765916462089\n",
      "loss: 0.0378286956177772\n",
      "loss: 0.02129245721025133\n",
      "loss: 0.04273626315768064\n",
      "step: 88\n",
      "loss: 0.022659067394133914\n",
      "loss: 0.03540783873533183\n",
      "loss: 0.06139357048311334\n",
      "loss: 0.10223617094514433\n",
      "loss: 0.03693280572711395\n",
      "loss: 0.020775691640447916\n",
      "loss: 0.04138741923073113\n",
      "step: 89\n",
      "loss: 0.022196768319520065\n",
      "loss: 0.034327161210749225\n",
      "loss: 0.05915055297316879\n",
      "loss: 0.09896485636732277\n",
      "loss: 0.03609237441925712\n",
      "loss: 0.020291166689328143\n",
      "loss: 0.04011746950469803\n",
      "step: 90\n",
      "loss: 0.021754793413293463\n",
      "loss: 0.03331430489912872\n",
      "loss: 0.057049508113163255\n",
      "loss: 0.09589303462732467\n",
      "loss: 0.03530087131855541\n",
      "loss: 0.01983571997064918\n",
      "loss: 0.0389197947796149\n",
      "step: 91\n",
      "loss: 0.021330977692840578\n",
      "loss: 0.032363172705133954\n",
      "loss: 0.05507968240179172\n",
      "loss: 0.09300268857039064\n",
      "loss: 0.03455247881961568\n",
      "loss: 0.019406480089193358\n",
      "loss: 0.037788454672171456\n",
      "step: 92\n",
      "loss: 0.020923442089195998\n",
      "loss: 0.03146826679658423\n",
      "loss: 0.05323114547907171\n",
      "loss: 0.09027802807839282\n",
      "loss: 0.03384209965266147\n",
      "loss: 0.019000855007553125\n",
      "loss: 0.036718110447964365\n",
      "step: 93\n",
      "loss: 0.02053057402571449\n",
      "loss: 0.03062463361517769\n",
      "loss: 0.051494723406593466\n",
      "loss: 0.08770513500471437\n",
      "loss: 0.03316533203868456\n",
      "loss: 0.018616517320332825\n",
      "loss: 0.03570395711834896\n",
      "step: 94\n",
      "loss: 0.02015100277708214\n",
      "loss: 0.02982781339239658\n",
      "loss: 0.04986194411927379\n",
      "loss: 0.08527167371154072\n",
      "loss: 0.03251842110923379\n",
      "loss: 0.018251387102496196\n",
      "loss: 0.03474166331567782\n",
      "step: 95\n",
      "loss: 0.019783571747386236\n",
      "loss: 0.029073793507413997\n",
      "loss: 0.04832499030631633\n",
      "loss: 0.08296665450010511\n",
      "loss: 0.0318981952046346\n",
      "loss: 0.01790361309583168\n",
      "loss: 0.03382731795383876\n",
      "step: 96\n",
      "loss: 0.019427309704139355\n",
      "loss: 0.02835896528647257\n",
      "loss: 0.04687665664618619\n",
      "loss: 0.08078023971126448\n",
      "loss: 0.031301994673685545\n",
      "loss: 0.01757155300299411\n",
      "loss: 0.032957382952084564\n",
      "step: 97\n",
      "loss: 0.01908140265804172\n",
      "loss: 0.02768008401546222\n",
      "loss: 0.04551030957037795\n",
      "loss: 0.07870358423385852\n",
      "loss: 0.030727599283297425\n",
      "loss: 0.017253753592749062\n",
      "loss: 0.0321286514332759\n",
      "step: 98\n",
      "loss: 0.0187451676243147\n",
      "loss: 0.027034232035728228\n",
      "loss: 0.044219848607621\n",
      "loss: 0.07672870373480294\n",
      "loss: 0.03017315867109402\n",
      "loss: 0.01694893120857078\n",
      "loss: 0.0313382108665645\n",
      "step: 99\n",
      "loss: 0.01841802904440153\n",
      "loss: 0.026418784836453865\n",
      "loss: 0.04299966893057127\n",
      "loss: 0.07484836520979453\n",
      "loss: 0.029637128700938573\n",
      "loss: 0.016655953140116222\n",
      "loss: 0.030583410651676926\n",
      "step: 100\n",
      "loss: 0.018099498250153837\n",
      "loss: 0.02583138006360136\n",
      "loss: 0.04184462506384255\n",
      "loss: 0.07305599550703444\n",
      "loss: 0.029118215263022766\n",
      "loss: 0.01637382018292555\n",
      "loss: 0.029861833661716006\n",
      "g.next= h\n",
      "abc.next= d\n"
     ]
    }
   ],
   "source": [
    "class RNNTest:\n",
    "\n",
    "    def __init__(self, hidden_num, all_chars):\n",
    "        '''\n",
    "        创建一个rnn\n",
    "        :param hidden_num: 隐层数目\n",
    "        :param all_chars: 所有字符集\n",
    "        '''\n",
    "        self.all_chars = all_chars\n",
    "        self.len = len(all_chars)\n",
    "        self.rnn = RNN(self.len, hidden_num, self.len)\n",
    "\n",
    "    def str2onehots(self, string):\n",
    "        '''\n",
    "        字符串转独热码\n",
    "        :param string:\n",
    "        :return:\n",
    "        '''\n",
    "        one_hots = []\n",
    "        for char in string:\n",
    "            one_hot = np.zeros((self.len,),dtype=np.int)\n",
    "            one_hot[self.all_chars.index(char)] = 1\n",
    "            one_hots.append(one_hot)\n",
    "        return one_hots\n",
    "\n",
    "    def vector2char(self, vector):\n",
    "        '''\n",
    "        预测向量转字符\n",
    "        :param vector:\n",
    "        :return:\n",
    "        '''\n",
    "        return self.all_chars[int(np.argmax(vector))]\n",
    "\n",
    "    def run(self, x_data, y_data, alpha=0.1, steps=100):\n",
    "\n",
    "        x_data_onehot = [self.str2onehots(xs) for xs in x_data]\n",
    "        y_data_onehot = [self.str2onehots(ys) for ys in y_data]\n",
    "        self.rnn.train(x_data_onehot, y_data_onehot, alpha=alpha, steps=steps) # 训练\n",
    "        vector_g = self.rnn.predict(self.str2onehots(\"g\"), False)[1] # 预测g下一个字母\n",
    "        vector_abc = self.rnn.predict(self.str2onehots(\"abc\"), False)[1] # 预测abc的下一个字母\n",
    "        print(\"g.next=\",self.vector2char(vector_g))\n",
    "        print(\"abc.next=\",self.vector2char(vector_abc))\n",
    "\n",
    "\n",
    "# 测试：下一个字母\n",
    "x_data = [\"abc\",\"bcd\",\"cdef\",\"fgh\",\"a\",\"bc\",\"abcdef\"]\n",
    "y_data = [\"bcd\",\"cde\",\"defg\",\"ghi\",\"b\",\"cd\",\"bcdefg\"]\n",
    "all_chars = \"abcdefghi\"\n",
    "\n",
    "rnn_test = RNNTest(10, all_chars)\n",
    "rnn_test.run(x_data,y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
